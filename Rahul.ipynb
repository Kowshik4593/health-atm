{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c3f8ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIDC-IDRI/manifest-1764432147192/LIDC-IDRI/LIDC-IDRI-0001\\01-01-2000-NA-NA-30178\\3000566.000000-NA-03192 133\n"
     ]
    }
   ],
   "source": [
    "import importlib.util\n",
    "from pathlib import Path\n",
    "\n",
    "# load module from a path because package/folder name contains a hyphen which is invalid in import statements\n",
    "module_path = Path(\"backend-dinesh/ml/preprocessing/select_series.py\")\n",
    "spec = importlib.util.spec_from_file_location(\"select_series\", module_path)\n",
    "select_series = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(select_series)\n",
    "\n",
    "find_main_ct_series = select_series.find_main_ct_series\n",
    "\n",
    "folder = \"LIDC-IDRI/manifest-1764432147192/LIDC-IDRI/LIDC-IDRI-0001\"\n",
    "series, count = find_main_ct_series(folder)\n",
    "print(series, count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3f50a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(133, 512, 512) [2.5, 0.703125, 0.703125]\n"
     ]
    }
   ],
   "source": [
    "# load the load_dicom module from the same preprocessing folder as select_series.py\n",
    "load_dicom_path = module_path.parent / \"load_dicom.py\"\n",
    "spec_ld = importlib.util.spec_from_file_location(\"ml.preprocessing.load_dicom\", load_dicom_path)\n",
    "load_dicom = importlib.util.module_from_spec(spec_ld)\n",
    "spec_ld.loader.exec_module(load_dicom)\n",
    "\n",
    "vol, spacing = load_dicom.load_dicom_series(series)\n",
    "print(vol.shape, spacing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b08ae67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(360, 360, 332) [1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "resample_path = module_path.parent / \"resample.py\"\n",
    "spec_rs = importlib.util.spec_from_file_location(\"ml.preprocessing.resample\", resample_path)\n",
    "resample = importlib.util.module_from_spec(spec_rs)\n",
    "spec_rs.loader.exec_module(resample)\n",
    "resampled_vol, new_spacing = resample.resample_to_iso(vol, spacing)\n",
    "print(resampled_vol.shape, new_spacing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8eb81aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/JoHof/lungmask/releases/download/v0.0/unet_r231-d5d2fc3d.pth\" to C:\\Users\\padal/.cache\\torch\\hub\\checkpoints\\unet_r231-d5d2fc3d.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119M/119M [00:27<00:00, 4.60MB/s] \n",
      "100%|██████████| 18/18.0 [00:05<00:00,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lungmask 2025-11-30 00:14:58 Postprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 56.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(360, 360, 332)\n"
     ]
    }
   ],
   "source": [
    "lung_segmentation_path = module_path.parent / \"lung_segmentation.py\"\n",
    "spec_ls = importlib.util.spec_from_file_location(\"ml.preprocessing.lung_segmentation\", lung_segmentation_path)\n",
    "lung_segmentation = importlib.util.module_from_spec(spec_ls)\n",
    "spec_ls.loader.exec_module(lung_segmentation)\n",
    "lung_mask = lung_segmentation.segment_lungs(resampled_vol)\n",
    "print(lung_mask.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ec6b370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidates found: 37751402\n",
      "[(np.int64(0), np.int64(0), np.int64(0)), (np.int64(0), np.int64(0), np.int64(1)), (np.int64(0), np.int64(0), np.int64(2)), (np.int64(0), np.int64(0), np.int64(3)), (np.int64(0), np.int64(0), np.int64(4)), (np.int64(0), np.int64(0), np.int64(5)), (np.int64(0), np.int64(0), np.int64(6)), (np.int64(0), np.int64(0), np.int64(7)), (np.int64(0), np.int64(0), np.int64(8)), (np.int64(0), np.int64(0), np.int64(9))]\n"
     ]
    }
   ],
   "source": [
    "# try the preprocessing folder first, then the detection folder sibling\n",
    "log_detector_path = module_path.parent / \"log_detector.py\"\n",
    "if not log_detector_path.exists():\n",
    "\talt_path = module_path.parent.parent / \"detection\" / \"log_detector.py\"\n",
    "\tif alt_path.exists():\n",
    "\t\tlog_detector_path = alt_path\n",
    "\telse:\n",
    "\t\traise FileNotFoundError(\n",
    "\t\t\tf\"log_detector.py not found at {module_path.parent} or {alt_path}. \"\n",
    "\t\t\t\"Please ensure the file exists or update the path.\"\n",
    "\t\t)\n",
    "\n",
    "spec_ld = importlib.util.spec_from_file_location(\"ml.detection.log_detector\", log_detector_path)\n",
    "log_detector = importlib.util.module_from_spec(spec_ld)\n",
    "spec_ld.loader.exec_module(log_detector)\n",
    "cands, logmap = log_detector.log_nodule_candidates(resampled_vol, lung_mask)\n",
    "print(\"Candidates found:\", len(cands))\n",
    "print(cands[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0c24c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered: 3522\n",
      "[(np.int64(60), np.int64(244), np.int64(104)), (np.int64(62), np.int64(245), np.int64(109)), (np.int64(64), np.int64(246), np.int64(113)), (np.int64(65), np.int64(239), np.int64(91)), (np.int64(65), np.int64(247), np.int64(118)), (np.int64(69), np.int64(245), np.int64(131)), (np.int64(70), np.int64(244), np.int64(107)), (np.int64(70), np.int64(246), np.int64(114)), (np.int64(70), np.int64(246), np.int64(118)), (np.int64(71), np.int64(240), np.int64(97))]\n"
     ]
    }
   ],
   "source": [
    "filter_candidates_path = module_path.parent / \"filter_candidates.py\"\n",
    "if not filter_candidates_path.exists():\n",
    "\talt_path = module_path.parent.parent / \"detection\" / \"filter_candidates.py\"\n",
    "\tif alt_path.exists():\n",
    "\t\tfilter_candidates_path = alt_path\n",
    "\telse:\n",
    "\t\traise FileNotFoundError(\n",
    "\t\t\tf\"filter_candidates.py not found at {module_path.parent} or {alt_path}. \"\n",
    "\t\t\t\"Please ensure the file exists or update the path.\"\n",
    "\t\t)\n",
    "spec_ld = importlib.util.spec_from_file_location(\"ml.detection.filter_candidates\", filter_candidates_path)\n",
    "filter_candidates = importlib.util.module_from_spec(spec_ld)\n",
    "spec_ld.loader.exec_module(filter_candidates)\n",
    "  \n",
    "filtered = filter_candidates.filter_candidates(cands, resampled_vol, lung_mask)\n",
    "print(\"Filtered:\", len(filtered))\n",
    "print(filtered[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78b37287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "patch_extractor_path = module_path.parent / \"patch_extractor.py\"\n",
    "if not patch_extractor_path.exists():\n",
    "\talt_path = module_path.parent.parent / \"detection\" / \"patch_extractor.py\"\n",
    "\tif alt_path.exists():\n",
    "\t\tpatch_extractor_path = alt_path\n",
    "\telse:\n",
    "\t\traise FileNotFoundError(\n",
    "\t\t\tf\"patch_extractor.py not found at {module_path.parent} or {alt_path}. \"\n",
    "\t\t\t\"Please ensure the file exists or update the path.\"\n",
    "\t\t)\n",
    "spec_ld = importlib.util.spec_from_file_location(\"ml.detection.patch_extractor\", patch_extractor_path)\n",
    "patch_extractor = importlib.util.module_from_spec(spec_ld)\n",
    "spec_ld.loader.exec_module(patch_extractor)\n",
    "\n",
    "patch = patch_extractor.extract_patch(resampled_vol, filtered[0])\n",
    "print(patch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b1e4bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3522"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load feature extractor and postprocess modules directly from files (package import may fail due to folder names)\n",
    "feature_extractor_path = module_path.parent.parent / \"features\" / \"feature_extractor.py\"\n",
    "if not feature_extractor_path.exists():\n",
    "    raise FileNotFoundError(f\"{feature_extractor_path} not found. Please check the path.\")\n",
    "\n",
    "spec_fe = importlib.util.spec_from_file_location(\"ml.features.feature_extractor\", feature_extractor_path)\n",
    "feature_extractor = importlib.util.module_from_spec(spec_fe)\n",
    "spec_fe.loader.exec_module(feature_extractor)\n",
    "compute_hu_stats = feature_extractor.compute_hu_stats\n",
    "compute_long_axis_mm = feature_extractor.compute_long_axis_mm\n",
    "compute_volume_mm3 = feature_extractor.compute_volume_mm3\n",
    "\n",
    "classify_type_path = module_path.parent.parent / \"postprocess\" / \"classify_type.py\"\n",
    "if not classify_type_path.exists():\n",
    "    raise FileNotFoundError(f\"{classify_type_path} not found. Please check the path.\")\n",
    "\n",
    "spec_ct = importlib.util.spec_from_file_location(\"ml.postprocess.classify_type\", classify_type_path)\n",
    "classify_type_mod = importlib.util.module_from_spec(spec_ct)\n",
    "spec_ct.loader.exec_module(classify_type_mod)\n",
    "classify_nodule_type = classify_type_mod.classify_nodule_type\n",
    "\n",
    "classify_lobe_path = module_path.parent.parent / \"postprocess\" / \"classify_lobe.py\"\n",
    "if not classify_lobe_path.exists():\n",
    "    raise FileNotFoundError(f\"{classify_lobe_path} not found. Please check the path.\")\n",
    "\n",
    "spec_cl = importlib.util.spec_from_file_location(\"ml.postprocess.classify_lobe\", classify_lobe_path)\n",
    "classify_lobe_mod = importlib.util.module_from_spec(spec_cl)\n",
    "spec_cl.loader.exec_module(classify_lobe_mod)\n",
    "classify_lobe = classify_lobe_mod.classify_lobe\n",
    "\n",
    "features = []\n",
    "\n",
    "for center in filtered:\n",
    "    patch = patch_extractor.extract_patch(resampled_vol, center)\n",
    "\n",
    "    # intensity\n",
    "    hu_mean, hu_std = compute_hu_stats(patch)\n",
    "\n",
    "    # morphology\n",
    "    long_axis = compute_long_axis_mm(patch, spacing=[1,1,1])\n",
    "    volume_mm3 = compute_volume_mm3(patch, spacing=[1,1,1])\n",
    "\n",
    "    # type\n",
    "    nodule_type = classify_nodule_type(hu_mean)\n",
    "\n",
    "    # location\n",
    "    lobe = classify_lobe(center, resampled_vol.shape)\n",
    "\n",
    "    features.append({\n",
    "        \"center\": center,\n",
    "        \"hu_mean\": hu_mean,\n",
    "        \"hu_std\": hu_std,\n",
    "        \"long_axis_mm\": long_axis,\n",
    "        \"volume_mm3\": volume_mm3,\n",
    "        \"type\": nodule_type,\n",
    "        \"lobe\": lobe\n",
    "    })\n",
    "\n",
    "len(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "944a4ea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('backend-dinesh/data/features/LIDC-IDRI-0001_features.csv')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "study_id = \"LIDC-IDRI-0001\"\n",
    "save_dir = module_path.parent.parent.parent / \"data\" / \"features\"\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "csv_path = save_dir / f\"{study_id}_features.csv\"\n",
    "\n",
    "with open(csv_path, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"center_z\", \"center_y\", \"center_x\",\n",
    "                     \"hu_mean\", \"hu_std\",\n",
    "                     \"long_axis_mm\", \"volume_mm3\",\n",
    "                     \"type\", \"lobe\"])\n",
    "    \n",
    "    for ft in features:\n",
    "        cz, cy, cx = ft[\"center\"]\n",
    "        writer.writerow([\n",
    "            int(cz), int(cy), int(cx),\n",
    "            ft[\"hu_mean\"], ft[\"hu_std\"],\n",
    "            ft[\"long_axis_mm\"], ft[\"volume_mm3\"],\n",
    "            ft[\"type\"], ft[\"lobe\"]\n",
    "        ])\n",
    "\n",
    "csv_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "426946ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 0.1402\n",
      "Epoch 2/10 - Loss: 0.0108\n",
      "Epoch 3/10 - Loss: 0.0199\n",
      "Epoch 4/10 - Loss: 0.0043\n",
      "Epoch 5/10 - Loss: 0.0106\n",
      "Epoch 6/10 - Loss: 0.0019\n",
      "Epoch 7/10 - Loss: 0.0016\n",
      "Epoch 8/10 - Loss: 0.0001\n",
      "Epoch 9/10 - Loss: 0.0008\n",
      "Epoch 10/10 - Loss: 0.0052\n",
      "Model saved: backend-dinesh\\models\\risk_head\\risk_head.pth\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "mlp_path = module_path.parent.parent / \"risk\" / \"train_mlp.py\"\n",
    "\n",
    "spec_mlp = importlib.util.spec_from_file_location(\"ml.risk.train_mlp\", mlp_path)\n",
    "mlp_mod = importlib.util.module_from_spec(spec_mlp)\n",
    "spec_mlp.loader.exec_module(mlp_mod)\n",
    "\n",
    "csv_dir = module_path.parent.parent.parent / \"data\" / \"features\"\n",
    "csv_files = list(csv_dir.glob(\"*_features.csv\"))\n",
    "\n",
    "save_dir = module_path.parent.parent.parent / \"models\" / \"risk_head\"\n",
    "model = mlp_mod.train_risk_mlp(csv_files, save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69ae0c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Risk: 1.0\n",
      "MC Risk: 1.0\n",
      "Entropy: -9.999999505838704e-08\n"
     ]
    }
   ],
   "source": [
    "risk_path = module_path.parent.parent / \"risk\" / \"predict_risk.py\"\n",
    "spec_r = importlib.util.spec_from_file_location(\"ml.risk.predict_risk\", risk_path)\n",
    "risk_mod = importlib.util.module_from_spec(spec_r)\n",
    "spec_r.loader.exec_module(risk_mod)\n",
    "\n",
    "model_dir = module_path.parent.parent.parent / \"models\" / \"risk_head\"\n",
    "risk_model = risk_mod.RiskHead(model_dir / \"risk_head.pth\", model_dir / \"risk_scaler.pkl\")\n",
    "\n",
    "# Test on first feature\n",
    "ft = features[0]\n",
    "vals = [ft[\"hu_mean\"], ft[\"hu_std\"], ft[\"long_axis_mm\"], ft[\"volume_mm3\"]]\n",
    "\n",
    "p = risk_model.predict(vals)\n",
    "p_mc, ent = risk_model.predict_mc_dropout(vals, T=5)\n",
    "\n",
    "print(\"Risk:\", p)\n",
    "print(\"MC Risk:\", p_mc)\n",
    "print(\"Entropy:\", ent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "042b18cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('backend-dinesh/outputs/LIDC-IDRI-0001_findings.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study_id = \"LIDC-IDRI-0001\"\n",
    "\n",
    "mal_scores = []\n",
    "uncs = []\n",
    "\n",
    "for ft in features:\n",
    "    vals = [ft[\"hu_mean\"], ft[\"hu_std\"], ft[\"long_axis_mm\"], ft[\"volume_mm3\"]]\n",
    "\n",
    "    p = risk_model.predict(vals)\n",
    "    p_mc, ent = risk_model.predict_mc_dropout(vals, T=5)\n",
    "\n",
    "    mal_scores.append(p)\n",
    "\n",
    "    uncs.append({\n",
    "        \"confidence\": p_mc,\n",
    "        \"entropy\": ent,\n",
    "        \"needs_review\": ent > 0.4\n",
    "    })\n",
    "\n",
    "builder_path = module_path.parent.parent / \"json_builder\" / \"builder.py\"\n",
    "spec_b = importlib.util.spec_from_file_location(\"ml.json_builder.builder\", builder_path)\n",
    "builder_mod = importlib.util.module_from_spec(spec_b)\n",
    "spec_b.loader.exec_module(builder_mod)\n",
    "\n",
    "save_json = module_path.parent.parent.parent / \"outputs\" / f\"{study_id}_findings.json\"\n",
    "save_json.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "builder_mod.build_findings_json(\n",
    "    study_id,\n",
    "    spacing=[1,1,1],\n",
    "    volume_shape=resampled_vol.shape,\n",
    "    filtered_candidates=filtered,\n",
    "    features=features,\n",
    "    malignancy_scores=mal_scores,\n",
    "    uncertainties=uncs,\n",
    "    output_path=str(save_json)\n",
    ")\n",
    "\n",
    "save_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7dad7e59",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'NoduleID'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\padal\\Documents\\Work\\FYP-1\\fypenv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mpandas/_libs/index.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/index.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'NoduleID'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m save_csv \u001b[38;5;241m=\u001b[39m module_path\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrisk_features\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlndb_features.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m save_csv\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 13\u001b[0m \u001b[43mlndb_extract\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_lndb_features\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlndb_root\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msave_csv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\padal\\Documents\\Work\\FYP-1\\backend-dinesh\\ml\\lndb\\extract_lndb_features.py:59\u001b[0m, in \u001b[0;36mextract_lndb_features\u001b[1;34m(lndb_root, save_csv)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m     58\u001b[0m     lndb_id \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLNDbID\u001b[39m\u001b[38;5;124m\"\u001b[39m]                \u001b[38;5;66;03m# 1,2,3,...\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m     nodule_id \u001b[38;5;241m=\u001b[39m \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNoduleID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m            \u001b[38;5;66;03m# 0,1,...\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     malignancy \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMalignancy\u001b[39m\u001b[38;5;124m\"\u001b[39m]         \u001b[38;5;66;03m# 0/1/2\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     vol_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLNDb-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(lndb_id)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m04d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m   \u001b[38;5;66;03m# LNDb-0001\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\padal\\Documents\\Work\\FYP-1\\fypenv\\lib\\site-packages\\pandas\\core\\series.py:1133\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[1;32mc:\\Users\\padal\\Documents\\Work\\FYP-1\\fypenv\\lib\\site-packages\\pandas\\core\\series.py:1249\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1248\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1249\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32mc:\\Users\\padal\\Documents\\Work\\FYP-1\\fypenv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3815\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3816\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3817\u001b[0m     ):\n\u001b[0;32m   3818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3821\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3822\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3823\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'NoduleID'"
     ]
    }
   ],
   "source": [
    "# Load module from file\n",
    "lndb_extract_path = module_path.parent.parent / \"lndb\" / \"extract_lndb_features.py\"\n",
    "\n",
    "spec_ex = importlib.util.spec_from_file_location(\"backend-dinesh.ml.lndb.extract\", lndb_extract_path)\n",
    "lndb_extract = importlib.util.module_from_spec(spec_ex)\n",
    "spec_ex.loader.exec_module(lndb_extract)\n",
    "\n",
    "# Run extraction\n",
    "lndb_root = Path(\"LNDBv4\")\n",
    "save_csv = module_path.parent.parent.parent / \"data\" / \"risk_features\" / \"lndb_features.csv\"\n",
    "save_csv.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "lndb_extract.extract_lndb_features(str(lndb_root), str(save_csv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "29ef93ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['LNDbID', 'RadID', 'RadFinding', 'FindingID', 'Nodule', 'x', 'y', 'z',\n",
      "       'DiamEq_Rad', 'Texture', 'Calcification', 'InternalStructure',\n",
      "       'Lobulation', 'Malignancy', 'Margin', 'Sphericity', 'Spiculation',\n",
      "       'Subtlety', 'Lobe', 'TextInstanceID', 'TextQuestion', 'Pos_Text',\n",
      "       'Diam_Text', 'NodType', 'Caract_Text', 'Where'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LNDbID</th>\n",
       "      <th>RadID</th>\n",
       "      <th>RadFinding</th>\n",
       "      <th>FindingID</th>\n",
       "      <th>Nodule</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>DiamEq_Rad</th>\n",
       "      <th>Texture</th>\n",
       "      <th>...</th>\n",
       "      <th>Spiculation</th>\n",
       "      <th>Subtlety</th>\n",
       "      <th>Lobe</th>\n",
       "      <th>TextInstanceID</th>\n",
       "      <th>TextQuestion</th>\n",
       "      <th>Pos_Text</th>\n",
       "      <th>Diam_Text</th>\n",
       "      <th>NodType</th>\n",
       "      <th>Caract_Text</th>\n",
       "      <th>Where</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1,2,3</td>\n",
       "      <td>1,1,1</td>\n",
       "      <td>1</td>\n",
       "      <td>1,1,1</td>\n",
       "      <td>-44.203451</td>\n",
       "      <td>-119.073242</td>\n",
       "      <td>-37.5</td>\n",
       "      <td>7.516572</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>apical</td>\n",
       "      <td>8.5</td>\n",
       "      <td>nod</td>\n",
       "      <td>margin: 1</td>\n",
       "      <td>TextReport+RadAnnotation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>25.852539</td>\n",
       "      <td>-126.969727</td>\n",
       "      <td>-45.5</td>\n",
       "      <td>6.626905</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RadAnnotation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1,2,3</td>\n",
       "      <td>1,1,3</td>\n",
       "      <td>1</td>\n",
       "      <td>1,1,1</td>\n",
       "      <td>88.895508</td>\n",
       "      <td>-123.867513</td>\n",
       "      <td>-129.5</td>\n",
       "      <td>8.971132</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>how many?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nod</td>\n",
       "      <td>calcification: 5</td>\n",
       "      <td>TextReport+RadAnnotation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1,3</td>\n",
       "      <td>2,2</td>\n",
       "      <td>2</td>\n",
       "      <td>1,1</td>\n",
       "      <td>63.534180</td>\n",
       "      <td>-112.756836</td>\n",
       "      <td>-117.5</td>\n",
       "      <td>6.937737</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>how many?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nod</td>\n",
       "      <td>calcification: 5</td>\n",
       "      <td>TextReport+RadAnnotation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1,3</td>\n",
       "      <td>3,5</td>\n",
       "      <td>3</td>\n",
       "      <td>1,1</td>\n",
       "      <td>-103.850586</td>\n",
       "      <td>-116.742188</td>\n",
       "      <td>-253.0</td>\n",
       "      <td>8.284458</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RadAnnotation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   LNDbID  RadID RadFinding  FindingID Nodule           x           y      z  \\\n",
       "0       1  1,2,3      1,1,1          1  1,1,1  -44.203451 -119.073242  -37.5   \n",
       "1       1      1          2          2      1   25.852539 -126.969727  -45.5   \n",
       "2       2  1,2,3      1,1,3          1  1,1,1   88.895508 -123.867513 -129.5   \n",
       "3       2    1,3        2,2          2    1,1   63.534180 -112.756836 -117.5   \n",
       "4       2    1,3        3,5          3    1,1 -103.850586 -116.742188 -253.0   \n",
       "\n",
       "   DiamEq_Rad  Texture  ...  Spiculation  Subtlety  Lobe  TextInstanceID  \\\n",
       "0    7.516572      5.0  ...     2.333333  3.666667     3             0.0   \n",
       "1    6.626905      4.0  ...     1.000000  3.000000     1             NaN   \n",
       "2    8.971132      5.0  ...     1.000000  5.000000     1             0.0   \n",
       "3    6.937737      5.0  ...     1.000000  5.000000     1             0.0   \n",
       "4    8.284458      5.0  ...     1.000000  3.500000     5             NaN   \n",
       "\n",
       "   TextQuestion  Pos_Text  Diam_Text  NodType       Caract_Text  \\\n",
       "0           NaN    apical        8.5      nod         margin: 1   \n",
       "1           NaN       NaN        NaN      NaN               NaN   \n",
       "2     how many?       NaN        NaN      nod  calcification: 5   \n",
       "3     how many?       NaN        NaN      nod  calcification: 5   \n",
       "4           NaN       NaN        NaN      NaN               NaN   \n",
       "\n",
       "                      Where  \n",
       "0  TextReport+RadAnnotation  \n",
       "1             RadAnnotation  \n",
       "2  TextReport+RadAnnotation  \n",
       "3  TextReport+RadAnnotation  \n",
       "4             RadAnnotation  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"LNDBv4/allNods.csv\")\n",
    "print(df.columns)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a1174c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainNodules columns:\n",
      " Index(['LNDbID', 'RadID', 'FindingID', 'x', 'y', 'z', 'Nodule', 'Volume',\n",
      "       'Text'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LNDbID</th>\n",
       "      <th>RadID</th>\n",
       "      <th>FindingID</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>Nodule</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-44.608398</td>\n",
       "      <td>-119.073242</td>\n",
       "      <td>-37.5</td>\n",
       "      <td>1</td>\n",
       "      <td>440.908794</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>25.852539</td>\n",
       "      <td>-126.969727</td>\n",
       "      <td>-45.5</td>\n",
       "      <td>1</td>\n",
       "      <td>152.381031</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-44.000977</td>\n",
       "      <td>-118.465820</td>\n",
       "      <td>-37.5</td>\n",
       "      <td>1</td>\n",
       "      <td>56.820045</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-44.000977</td>\n",
       "      <td>-119.680664</td>\n",
       "      <td>-37.5</td>\n",
       "      <td>1</td>\n",
       "      <td>169.353252</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>88.895508</td>\n",
       "      <td>-123.625977</td>\n",
       "      <td>-129.5</td>\n",
       "      <td>1</td>\n",
       "      <td>339.187950</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LNDbID  RadID  FindingID          x           y      z  Nodule      Volume  \\\n",
       "0       1      1          1 -44.608398 -119.073242  -37.5       1  440.908794   \n",
       "1       1      1          2  25.852539 -126.969727  -45.5       1  152.381031   \n",
       "2       1      2          1 -44.000977 -118.465820  -37.5       1   56.820045   \n",
       "3       1      3          1 -44.000977 -119.680664  -37.5       1  169.353252   \n",
       "4       2      1          1  88.895508 -123.625977 -129.5       1  339.187950   \n",
       "\n",
       "   Text  \n",
       "0     5  \n",
       "1     4  \n",
       "2     5  \n",
       "3     5  \n",
       "4     5  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nod = pd.read_csv(z.open(\"trainNodules.csv\"))\n",
    "print(\"trainNodules columns:\\n\", df_nod.columns)\n",
    "df_nod.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "175681b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "trainNodules_gt columns:\n",
      " Index(['LNDbID', 'RadID', 'RadFindingID', 'FindingID', 'x', 'y', 'z',\n",
      "       'AgrLevel', 'Nodule', 'Volume', 'Text'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LNDbID</th>\n",
       "      <th>RadID</th>\n",
       "      <th>RadFindingID</th>\n",
       "      <th>FindingID</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>AgrLevel</th>\n",
       "      <th>Nodule</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1,2,3</td>\n",
       "      <td>1,1,1</td>\n",
       "      <td>1</td>\n",
       "      <td>-44.203451</td>\n",
       "      <td>-119.073242</td>\n",
       "      <td>-37.5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>222.360697</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.852539</td>\n",
       "      <td>-126.969727</td>\n",
       "      <td>-45.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>152.381031</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1,2,3</td>\n",
       "      <td>1,1,3</td>\n",
       "      <td>1</td>\n",
       "      <td>88.895508</td>\n",
       "      <td>-123.867513</td>\n",
       "      <td>-129.5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>378.042297</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1,3</td>\n",
       "      <td>2,2</td>\n",
       "      <td>2</td>\n",
       "      <td>63.534180</td>\n",
       "      <td>-112.756836</td>\n",
       "      <td>-117.5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>174.844563</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1,3</td>\n",
       "      <td>3,5</td>\n",
       "      <td>3</td>\n",
       "      <td>-103.850586</td>\n",
       "      <td>-116.742188</td>\n",
       "      <td>-253.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>297.708309</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LNDbID  RadID RadFindingID  FindingID           x           y      z  \\\n",
       "0       1  1,2,3        1,1,1          1  -44.203451 -119.073242  -37.5   \n",
       "1       1      1            2          2   25.852539 -126.969727  -45.5   \n",
       "2       2  1,2,3        1,1,3          1   88.895508 -123.867513 -129.5   \n",
       "3       2    1,3          2,2          2   63.534180 -112.756836 -117.5   \n",
       "4       2    1,3          3,5          3 -103.850586 -116.742188 -253.0   \n",
       "\n",
       "   AgrLevel  Nodule      Volume  Text  \n",
       "0         3       1  222.360697   5.0  \n",
       "1         1       1  152.381031   4.0  \n",
       "2         3       1  378.042297   5.0  \n",
       "3         2       1  174.844563   5.0  \n",
       "4         2       1  297.708309   5.0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gt = pd.read_csv(z.open(\"trainNodules_gt.csv\"))\n",
    "print(\"\\ntrainNodules_gt columns:\\n\", df_gt.columns)\n",
    "df_gt.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80af94cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainNodules columns: Index(['LNDbID', 'RadID', 'FindingID', 'x', 'y', 'z', 'Nodule', 'Volume',\n",
      "       'Text'],\n",
      "      dtype='object')\n",
      "\n",
      "trainNodules_gt columns: Index(['LNDbID', 'RadID', 'RadFindingID', 'FindingID', 'x', 'y', 'z',\n",
      "       'AgrLevel', 'Nodule', 'Volume', 'Text'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LNDbID</th>\n",
       "      <th>RadID</th>\n",
       "      <th>RadFindingID</th>\n",
       "      <th>FindingID</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>AgrLevel</th>\n",
       "      <th>Nodule</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1,2,3</td>\n",
       "      <td>1,1,1</td>\n",
       "      <td>1</td>\n",
       "      <td>-44.203451</td>\n",
       "      <td>-119.073242</td>\n",
       "      <td>-37.5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>222.360697</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.852539</td>\n",
       "      <td>-126.969727</td>\n",
       "      <td>-45.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>152.381031</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1,2,3</td>\n",
       "      <td>1,1,3</td>\n",
       "      <td>1</td>\n",
       "      <td>88.895508</td>\n",
       "      <td>-123.867513</td>\n",
       "      <td>-129.5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>378.042297</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1,3</td>\n",
       "      <td>2,2</td>\n",
       "      <td>2</td>\n",
       "      <td>63.534180</td>\n",
       "      <td>-112.756836</td>\n",
       "      <td>-117.5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>174.844563</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1,3</td>\n",
       "      <td>3,5</td>\n",
       "      <td>3</td>\n",
       "      <td>-103.850586</td>\n",
       "      <td>-116.742188</td>\n",
       "      <td>-253.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>297.708309</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LNDbID  RadID RadFindingID  FindingID           x           y      z  \\\n",
       "0       1  1,2,3        1,1,1          1  -44.203451 -119.073242  -37.5   \n",
       "1       1      1            2          2   25.852539 -126.969727  -45.5   \n",
       "2       2  1,2,3        1,1,3          1   88.895508 -123.867513 -129.5   \n",
       "3       2    1,3          2,2          2   63.534180 -112.756836 -117.5   \n",
       "4       2    1,3          3,5          3 -103.850586 -116.742188 -253.0   \n",
       "\n",
       "   AgrLevel  Nodule      Volume  Text  \n",
       "0         3       1  222.360697   5.0  \n",
       "1         1       1  152.381031   4.0  \n",
       "2         3       1  378.042297   5.0  \n",
       "3         2       1  174.844563   5.0  \n",
       "4         2       1  297.708309   5.0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nod = pd.read_csv(z.open(\"trainNodules.csv\"))\n",
    "print(\"trainNodules columns:\", df_nod.columns)\n",
    "df_nod.head()\n",
    "\n",
    "df_gt = pd.read_csv(z.open(\"trainNodules_gt.csv\"))\n",
    "print(\"\\ntrainNodules_gt columns:\", df_gt.columns)\n",
    "df_gt.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "064328d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: backend-dinesh/data/risk_features/lndb_features.csv\n"
     ]
    }
   ],
   "source": [
    "spec = importlib.util.spec_from_file_location(\"lndb_fast\", Path(\"backend-dinesh/ml/lndb/extract_lndb_fast.py\"))\n",
    "mod = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(mod)\n",
    "mod.extract_lndb_fast(\"LNDBv4\", \"backend-dinesh/data/risk_features/lndb_features.csv\", radius_mm=8.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c9892785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12 - Loss: 0.5904\n",
      "Epoch 2/12 - Loss: 0.4712\n",
      "Epoch 3/12 - Loss: 0.3451\n",
      "Epoch 4/12 - Loss: 0.2499\n",
      "Epoch 5/12 - Loss: 0.1899\n",
      "Epoch 6/12 - Loss: 0.1751\n",
      "Epoch 7/12 - Loss: 0.1665\n",
      "Epoch 8/12 - Loss: 0.1591\n",
      "Epoch 9/12 - Loss: 0.1910\n",
      "Epoch 10/12 - Loss: 0.1651\n",
      "Epoch 11/12 - Loss: 0.1605\n",
      "Epoch 12/12 - Loss: 0.1648\n",
      "Saved model & scaler to backend-dinesh\\models\\risk_head\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4, out_features=32, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Dropout(p=0.2, inplace=False)\n",
       "  (3): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (4): ReLU()\n",
       "  (5): Dropout(p=0.1, inplace=False)\n",
       "  (6): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (7): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec = importlib.util.spec_from_file_location(\"train_lndb\", Path(\"backend-dinesh/ml/risk/train_lndb_mlp.py\"))\n",
    "mod = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(mod)\n",
    "mod.train_lndb_mlp(\"backend-dinesh/data/risk_features/lndb_features.csv\", \"backend-dinesh/models/risk_head\", epochs=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6dab43a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold AUCs: [0.5853427895981087, 0.5513002364066193, 0.5200945626477541, 0.5702127659574469, 0.5101063829787233] mean AUC: 0.5474113475177305\n",
      "Saved ROC & calibration to outputs/metrics/\n",
      "Saved ROC & calibration to outputs/metrics/\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, brier_score_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib, torch\n",
    "\n",
    "df = pd.read_csv(\"backend-dinesh/data/risk_features/lndb_features.csv\")\n",
    "X = df[[\"hu_mean\",\"hu_std\",\"long_axis_mm\",\"volume_mm3\"]].fillna(0).values.astype(np.float32)\n",
    "y = df[\"malignancy\"].astype(int).values\n",
    "scaler = joblib.load(\"backend-dinesh/models/risk_head/risk_scaler.pkl\")\n",
    "model_state = torch.load(\"backend-dinesh/models/risk_head/risk_head.pth\", map_location=\"cpu\")\n",
    "\n",
    "# recreate model\n",
    "import torch.nn as nn\n",
    "model = nn.Sequential(nn.Linear(4,32), nn.ReLU(), nn.Dropout(0.2), nn.Linear(32,16), nn.ReLU(), nn.Dropout(0.1), nn.Linear(16,1), nn.Sigmoid())\n",
    "model.load_state_dict(model_state)\n",
    "model.eval()\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "aucs = []\n",
    "for train_idx, test_idx in skf.split(X,y):\n",
    "    Xtr, Xte = scaler.transform(X[train_idx]), scaler.transform(X[test_idx])\n",
    "    with torch.no_grad():\n",
    "        preds = model(torch.tensor(Xte, dtype=torch.float32)).numpy().ravel()\n",
    "    aucs.append(roc_auc_score(y[test_idx], preds))\n",
    "print(\"5-fold AUCs:\", aucs, \"mean AUC:\", np.mean(aucs))\n",
    "\n",
    "# save ROC of last fold\n",
    "fpr, tpr, _ = roc_curve(y[test_idx], preds)\n",
    "plt.figure(); plt.plot(fpr,tpr); plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC\"); plt.grid(True)\n",
    "out_dir = Path(\"outputs/metrics\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(str(out_dir / \"roc_lndb.png\"), bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# calibration curve (binned)\n",
    "from sklearn.calibration import calibration_curve\n",
    "prob_true, prob_pred = calibration_curve(y[test_idx], preds, n_bins=10)\n",
    "plt.figure(); plt.plot(prob_pred, prob_true, marker='o'); plt.plot([0,1],[0,1], '--'); plt.xlabel(\"Predicted\"); plt.ylabel(\"Observed\"); plt.title(\"Calibration\"); plt.grid(True)\n",
    "plt.savefig(str(out_dir / \"calibration_lndb.png\"), bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "print(\"Saved ROC & calibration to outputs/metrics/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dde18560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prob</th>\n",
       "      <th>mc</th>\n",
       "      <th>entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.000000e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.000000e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.000000e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.000000e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.000000e-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   prob   mc       entropy\n",
       "0   0.0  0.0 -1.000000e-09\n",
       "1   0.0  0.0 -1.000000e-09\n",
       "2   0.0  0.0 -1.000000e-09\n",
       "3   0.0  0.0 -1.000000e-09\n",
       "4   0.0  0.0 -1.000000e-09"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib, torch, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "scaler = joblib.load(\"backend-dinesh/models/risk_head/risk_scaler.pkl\")\n",
    "state = torch.load(\"backend-dinesh/models/risk_head/risk_head.pth\", map_location=\"cpu\")\n",
    "import torch.nn as nn\n",
    "model = nn.Sequential(nn.Linear(4,32), nn.ReLU(), nn.Dropout(0.2), nn.Linear(32,16), nn.ReLU(), nn.Dropout(0.1), nn.Linear(16,1), nn.Sigmoid())\n",
    "model.load_state_dict(state); model.eval()\n",
    "\n",
    "def predict_prob(feat_row, T=8):\n",
    "    x = np.array([feat_row[\"hu_mean\"], feat_row[\"hu_std\"], feat_row[\"long_axis_mm\"], feat_row[\"volume_mm3\"]], dtype=np.float32).reshape(1,-1)\n",
    "    x_s = scaler.transform(x)\n",
    "    x_t = torch.tensor(x_s, dtype=torch.float32)\n",
    "    # deterministic\n",
    "    with torch.no_grad():\n",
    "        p = model(x_t).item()\n",
    "    # MC-dropout (enable dropout layers)\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Dropout): m.train()\n",
    "    probs = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(T):\n",
    "            probs.append(model(x_t).item())\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Dropout): m.eval()\n",
    "    probs = np.array(probs)\n",
    "    return p, float(probs.mean()), float(-(probs.mean()*np.log(probs.mean()+1e-9) + (1-probs.mean())*np.log(1-probs.mean()+1e-9)))\n",
    "\n",
    "# example: run on LIDC features file\n",
    "df = pd.read_csv(\"backend-dinesh/data/features/LIDC-IDRI-0001_features.csv\")\n",
    "out = []\n",
    "for _, r in df.iterrows():\n",
    "    p_det, p_mc, ent = predict_prob(r, T=8)\n",
    "    out.append({\"prob\":p_det, \"mc\":p_mc, \"entropy\":ent})\n",
    "pd.DataFrame(out).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b0151b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "study_id: LIDC-IDRI-0001\n",
      "num_candidates: 93\n",
      "num_nodules: 93\n",
      "processing_time_seconds: 6.125739574432373\n",
      "lung fields: Lungs appear within expected attenuation ranges. 0.0010729492042243047 0.9141988788487282\n",
      "first nodule keys: ['id', 'centroid', 'coordinates', 'bbox', 'mask_path', 'long_axis_mm', 'volume_mm3', 'type', 'lobe', 'location', 'prob_malignant', 'uncertainty']\n",
      "first nodule prob: 0.95\n",
      "first nodule uncertainty: {'confidence': 0.9945688275300512, 'entropy': 0.03374321933759373, 'needs_review': False}\n",
      "first nodule location field: right middle lobe lobe: right middle lobe\n",
      "sanity ratio long_axis_mm(mm)^3 vs volume_mm3: 78139.0522579655 32071.0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "j = json.load(open(\"backend-dinesh/outputs/LIDC-IDRI-0001_findings.json\"))\n",
    "# basic checks\n",
    "print(\"study_id:\", j.get(\"study_id\"))\n",
    "print(\"num_candidates:\", j.get(\"num_candidates\"))\n",
    "print(\"num_nodules:\", j.get(\"num_nodules\"))\n",
    "print(\"processing_time_seconds:\", j.get(\"processing_time_seconds\"))\n",
    "print(\"lung fields:\", j.get(\"lung_health\"), j.get(\"emphysema_score\"), j.get(\"consolidation_score\"))\n",
    "n0 = j[\"nodules\"][0] if j[\"nodules\"] else None\n",
    "print(\"first nodule keys:\", list(n0.keys()) if n0 else None)\n",
    "print(\"first nodule prob:\", n0.get(\"prob_malignant\") if n0 else None)\n",
    "print(\"first nodule uncertainty:\", n0.get(\"uncertainty\") if n0 else None)\n",
    "print(\"first nodule location field:\", n0.get(\"location\"), \"lobe:\", n0.get(\"lobe\"))\n",
    "# quick sanity: long axis vs volume\n",
    "if n0:\n",
    "    la = n0.get(\"long_axis_mm\",0)\n",
    "    vol = n0.get(\"volume_mm3\",0)\n",
    "    print(\"sanity ratio long_axis_mm(mm)^3 vs volume_mm3:\", la**3, vol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10b3087d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "study_id: LIDC-IDRI-0001\n",
      "num_nodules: 93\n",
      "risk min: 0.9\n",
      "risk max: 0.9\n",
      "unique risks: 1\n",
      "types: {'subsolid', 'solid', 'ground-glass'}\n",
      "lobes: {'left lower lobe', 'left middle lobe', 'right middle lobe', 'right lower lobe', 'right upper lobe'}\n",
      "needs_review count: 0\n",
      "42.751961609058235 32071.0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "j = json.load(open(\"backend-dinesh/outputs/LIDC-IDRI-0001_findings.json\"))\n",
    "\n",
    "# Key Phase-1 checks:\n",
    "\n",
    "print(\"study_id:\", j[\"study_id\"])\n",
    "print(\"num_nodules:\", j[\"num_nodules\"])\n",
    "\n",
    "# 1) Check risk variation\n",
    "risks = [n[\"prob_malignant\"] for n in j[\"nodules\"]]\n",
    "print(\"risk min:\", min(risks))\n",
    "print(\"risk max:\", max(risks))\n",
    "print(\"unique risks:\", len(set(round(r,3) for r in risks)))\n",
    "\n",
    "# 2) Check type values\n",
    "types = set(n[\"type\"] for n in j[\"nodules\"])\n",
    "print(\"types:\", types)\n",
    "\n",
    "# 3) Check lobe values\n",
    "lobes = set(n[\"location\"] for n in j[\"nodules\"])\n",
    "print(\"lobes:\", lobes)\n",
    "\n",
    "# 4) Check uncertainty\n",
    "unc = [n[\"uncertainty\"][\"needs_review\"] for n in j[\"nodules\"]]\n",
    "print(\"needs_review count:\", sum(unc))\n",
    "\n",
    "# 5) Size-volume sanity\n",
    "first = j[\"nodules\"][0]\n",
    "print(first[\"long_axis_mm\"], first[\"volume_mm3\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe3d20b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9 0.9 1\n",
      "{'subsolid', 'solid', 'ground-glass'}\n",
      "{'left lower lobe', 'left middle lobe', 'right middle lobe', 'right lower lobe', 'right upper lobe'}\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "j = json.load(open(\"backend-dinesh/outputs/LIDC-IDRI-0001_findings.json\"))\n",
    "\n",
    "risks = [n[\"prob_malignant\"] for n in j[\"nodules\"]]\n",
    "print(min(risks), max(risks), len(set(risks)))\n",
    "\n",
    "print(set(n[\"type\"] for n in j[\"nodules\"]))\n",
    "print(set(n[\"location\"] for n in j[\"nodules\"]))\n",
    "print(sum(n[\"uncertainty\"][\"needs_review\"] for n in j[\"nodules\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e2e2f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "risk min: 0.9\n",
      "risk max: 0.9\n",
      "unique risks: 1\n",
      "types: {'subsolid', 'solid', 'ground-glass'}\n",
      "lobes: {'left lower lobe', 'right lower lobe', 'right middle lobe', 'right upper lobe', 'left lingula'}\n",
      "needs_review count: 0\n",
      "long_axis vs volume sample: 42.751961609058235 32071.0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "j = json.load(open(\"backend-dinesh/outputs/LIDC-IDRI-0001_findings.json\"))\n",
    "\n",
    "risks = [n[\"prob_malignant\"] for n in j[\"nodules\"]]\n",
    "types = {n[\"type\"] for n in j[\"nodules\"]}\n",
    "lobes = {n[\"location\"] for n in j[\"nodules\"]}\n",
    "needs = [n[\"uncertainty\"][\"needs_review\"] for n in j[\"nodules\"]]\n",
    "\n",
    "print(\"risk min:\", min(risks))\n",
    "print(\"risk max:\", max(risks))\n",
    "print(\"unique risks:\", len(set([round(r,4) for r in risks])))\n",
    "\n",
    "print(\"types:\", types)\n",
    "print(\"lobes:\", lobes)\n",
    "print(\"needs_review count:\", sum(needs))\n",
    "\n",
    "print(\"long_axis vs volume sample:\",\n",
    "      j[\"nodules\"][0][\"long_axis_mm\"],\n",
    "      j[\"nodules\"][0][\"volume_mm3\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "086bfc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "risk min: 0.6447073954119117\n",
      "risk max: 0.9\n",
      "unique risks: 93\n",
      "types: {'subsolid', 'solid', 'ground-glass'}\n",
      "lobes: {'left lower lobe', 'right lower lobe', 'right middle lobe', 'right upper lobe', 'left lingula'}\n",
      "needs_review count: 92\n",
      "sample long_axis, vol: 42.751961609058235 32071.0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "j = json.load(open(\"backend-dinesh/outputs/LIDC-IDRI-0001_findings.json\"))\n",
    "risks = [n[\"prob_malignant\"] for n in j[\"nodules\"]]\n",
    "print(\"risk min:\", min(risks))\n",
    "print(\"risk max:\", max(risks))\n",
    "print(\"unique risks:\", len(set([round(r,4) for r in risks])))\n",
    "print(\"types:\", set(n[\"type\"] for n in j[\"nodules\"]))\n",
    "print(\"lobes:\", set(n[\"location\"] for n in j[\"nodules\"]))\n",
    "print(\"needs_review count:\", sum(n[\"uncertainty\"][\"needs_review\"] for n in j[\"nodules\"]))\n",
    "print(\"sample long_axis, vol:\", j[\"nodules\"][0][\"long_axis_mm\"], j[\"nodules\"][0][\"volume_mm3\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d472a01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "SUPABASE_URL = os.getenv(\"SUPABASE_URL\")\n",
    "SUPABASE_ANON_KEY = os.getenv(\"SUPABASE_ANON_KEY\")\n",
    "SUPABASE_SERVICE_KEY = os.getenv(\"SUPABASE_SERVICE_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e17f3216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: 200\n",
      "{'case_id': '5097f192-e3f6-4514-8a77-9199983a750c', 'storage_path': '5097f192-e3f6-4514-8a77-9199983a750c/test_ct.zip'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "BASE_URL = \"http://127.0.0.1:8000\"\n",
    "\n",
    "headers = {\n",
    "    \"x-user-id\": \"5e5202af-44ad-4994-9515-ba2fb3eb4dec\",\n",
    "    \"x-user-role\": \"patient\"\n",
    "}\n",
    "\n",
    "zip_path = \"test_ct.zip\"\n",
    "\n",
    "files = {\n",
    "    \"file\": (\"test_ct.zip\", open(zip_path, \"rb\"), \"application/zip\")\n",
    "}\n",
    "\n",
    "resp = requests.post(f\"{BASE_URL}/upload/scan\", headers=headers, files=files)\n",
    "\n",
    "print(\"Status:\", resp.status_code)\n",
    "try:\n",
    "    print(resp.json())\n",
    "except:\n",
    "    print(resp.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b71c328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: 500\n",
      "{\"detail\":\"Pipeline error: Pipeline not found at: C:\\\\Users\\\\padal\\\\Documents\\\\Work\\\\FYP-1\\\\backend-dinesh\\\\backend-dinesh\\\\ml\\\\pipeline.py\"}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "BASE_URL = \"http://127.0.0.1:8000\"\n",
    "\n",
    "case_id = \"5097f192-e3f6-4514-8a77-9199983a750c\"\n",
    "\n",
    "resp = requests.post(\n",
    "    f\"{BASE_URL}/process/case/{case_id}\",\n",
    "    headers={\n",
    "        \"x-user-id\": \"6255e461-2a14-4b3c-a635-de4428668fc0\",  # operator\n",
    "        \"x-user-role\": \"operator\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Status:\", resp.status_code)\n",
    "print(resp.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de8912dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 {\"status\":\"completed\"}\n"
     ]
    }
   ],
   "source": [
    "resp = requests.post(\n",
    "    \"http://127.0.0.1:8000/process/case/5097f192-e3f6-4514-8a77-9199983a750c\",\n",
    "    headers={\n",
    "        \"x-user-id\": \"6255e461-2a14-4b3c-a635-de4428668fc0\",\n",
    "        \"x-user-role\": \"operator\"\n",
    "    }\n",
    ")\n",
    "print(resp.status_code, resp.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4c37606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 {\"status\":\"completed\"}\n"
     ]
    }
   ],
   "source": [
    "resp = requests.post(\"http://127.0.0.1:8000/process/case/5097f192-e3f6-4514-8a77-9199983a750c\",\n",
    "                     headers={\"x-user-id\": \"6255e461-2a14-4b3c-a635-de4428668fc0\", \"x-user-role\":\"operator\"})\n",
    "print(resp.status_code, resp.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b813b196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: 200\n",
      "{\"status\":\"completed\"}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "resp = requests.post(\n",
    "    \"http://127.0.0.1:8000/process/case/5097f192-e3f6-4514-8a77-9199983a750c\",\n",
    "    headers={\n",
    "        \"x-user-id\": \"6255e461-2a14-4b3c-a635-de4428668fc0\",  # operator\n",
    "        \"x-user-role\": \"operator\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Status:\", resp.status_code)\n",
    "print(resp.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f84da19",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATIENT_ID = \"5e5202af-44ad-4994-9515-ba2fb3eb4dec\"\n",
    "OPERATOR_ID = \"6255e461-2a14-4b3c-a635-de4428668fc0\"\n",
    "DOCTOR_ID   = \"358f00bb-d2f3-4352-b9bd-29042d3e9c57\"\n",
    "FILE_PATH   = \"test_ct_extracted.zip\"\n",
    "BACKEND_URL = \"http://127.0.0.1:8000\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c79f8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: 200\n",
      "Response: {\"case_id\":\"99d04c3e-c389-4dd3-9939-a78029f69886\",\"storage_path\":\"99d04c3e-c389-4dd3-9939-a78029f69886/test_ct.zip\"}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "BASE = \"http://127.0.0.1:8000\"\n",
    "\n",
    "patient_id = \"5e5202af-44ad-4994-9515-ba2fb3eb4dec\"\n",
    "zip_path = \"test_ct_extracted.zip\"\n",
    "\n",
    "headers = {\n",
    "    \"x-user-id\": patient_id,\n",
    "    \"x-user-role\": \"patient\",\n",
    "}\n",
    "\n",
    "# STEP 1 — upload\n",
    "with open(zip_path, \"rb\") as f:\n",
    "    r = requests.post(\n",
    "        f\"{BASE}/upload/scan\",\n",
    "        headers=headers,\n",
    "        files={\"file\": (\"test_ct.zip\", f, \"application/zip\")}\n",
    "    )\n",
    "\n",
    "print(\"Status:\", r.status_code)\n",
    "print(\"Response:\", r.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bd23e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: 200\n",
      "{\"status\":\"completed\"}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "BASE = \"http://127.0.0.1:8000\"\n",
    "operator_id = \"6255e461-2a14-4b3c-a635-de4428668fc0\"\n",
    "case_id = \"99d04c3e-c389-4dd3-9939-a78029f69886\"\n",
    "\n",
    "headers = {\n",
    "    \"x-user-id\": operator_id,\n",
    "    \"x-user-role\": \"operator\"\n",
    "}\n",
    "\n",
    "r = requests.post(f\"{BASE}/process/case/{case_id}\", headers=headers)\n",
    "print(\"Status:\", r.status_code)\n",
    "print(r.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92ea6c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "[{'id': '1774e42a-c1b2-4743-984a-934bd7e9a4fc', 'patient_id': '5e5202af-44ad-4994-9515-ba2fb3eb4dec', 'uploaded_at': '2025-11-30T14:51:07.187026+00:00', 'storage_path': 'pending', 'status': 'uploaded', 'updated_at': '2025-11-30T14:51:07.187026+00:00'}, {'id': '060b6466-8a5b-45ae-b348-5d0c6864c5d3', 'patient_id': '5e5202af-44ad-4994-9515-ba2fb3eb4dec', 'uploaded_at': '2025-11-30T14:53:52.696452+00:00', 'storage_path': 'pending', 'status': 'uploaded', 'updated_at': '2025-11-30T14:53:52.696452+00:00'}, {'id': '5097f192-e3f6-4514-8a77-9199983a750c', 'patient_id': '5e5202af-44ad-4994-9515-ba2fb3eb4dec', 'uploaded_at': '2025-11-30T15:02:57.222918+00:00', 'storage_path': '5097f192-e3f6-4514-8a77-9199983a750c/test_ct.zip', 'status': 'processing', 'updated_at': '2025-11-30T16:25:20.970227+00:00'}, {'id': '99d04c3e-c389-4dd3-9939-a78029f69886', 'patient_id': '5e5202af-44ad-4994-9515-ba2fb3eb4dec', 'uploaded_at': '2025-11-30T18:11:30.922211+00:00', 'storage_path': '99d04c3e-c389-4dd3-9939-a78029f69886/test_ct.zip', 'status': 'processing', 'updated_at': '2025-11-30T18:16:15.607309+00:00'}]\n"
     ]
    }
   ],
   "source": [
    "patient_id = \"5e5202af-44ad-4994-9515-ba2fb3eb4dec\"\n",
    "\n",
    "headers = {\n",
    "    \"x-user-id\": patient_id,\n",
    "    \"x-user-role\": \"patient\"\n",
    "}\n",
    "\n",
    "r = requests.get(f\"{BASE}/cases/patient/{patient_id}\", headers=headers)\n",
    "print(r.status_code)\n",
    "print(r.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f7ebd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: 200\n",
      "{'status': 'completed'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "BASE = \"http://127.0.0.1:8000\"\n",
    "\n",
    "case_id = \"99d04c3e-c389-4dd3-9939-a78029f69886\"\n",
    "operator_id = \"6255e461-2a14-4b3c-a635-de4428668fc0\"\n",
    "\n",
    "headers = {\n",
    "    \"x-user-id\": operator_id,\n",
    "    \"x-user-role\": \"operator\"\n",
    "}\n",
    "\n",
    "r = requests.post(f\"{BASE}/process/case/{case_id}\", headers=headers)\n",
    "print(\"Status:\", r.status_code)\n",
    "print(r.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "316685e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: 200\n",
      "[{'id': '1774e42a-c1b2-4743-984a-934bd7e9a4fc', 'patient_id': '5e5202af-44ad-4994-9515-ba2fb3eb4dec', 'uploaded_at': '2025-11-30T14:51:07.187026+00:00', 'storage_path': 'pending', 'status': 'uploaded', 'updated_at': '2025-11-30T14:51:07.187026+00:00'}, {'id': '060b6466-8a5b-45ae-b348-5d0c6864c5d3', 'patient_id': '5e5202af-44ad-4994-9515-ba2fb3eb4dec', 'uploaded_at': '2025-11-30T14:53:52.696452+00:00', 'storage_path': 'pending', 'status': 'uploaded', 'updated_at': '2025-11-30T14:53:52.696452+00:00'}, {'id': '5097f192-e3f6-4514-8a77-9199983a750c', 'patient_id': '5e5202af-44ad-4994-9515-ba2fb3eb4dec', 'uploaded_at': '2025-11-30T15:02:57.222918+00:00', 'storage_path': '5097f192-e3f6-4514-8a77-9199983a750c/test_ct.zip', 'status': 'processing', 'updated_at': '2025-11-30T16:25:20.970227+00:00'}, {'id': '99d04c3e-c389-4dd3-9939-a78029f69886', 'patient_id': '5e5202af-44ad-4994-9515-ba2fb3eb4dec', 'uploaded_at': '2025-11-30T18:11:30.922211+00:00', 'storage_path': '99d04c3e-c389-4dd3-9939-a78029f69886/test_ct.zip', 'status': 'processing', 'updated_at': '2025-11-30T18:47:01.983027+00:00'}]\n"
     ]
    }
   ],
   "source": [
    "patient_id = \"5e5202af-44ad-4994-9515-ba2fb3eb4dec\"\n",
    "\n",
    "headers = {\n",
    "    \"x-user-id\": patient_id,\n",
    "    \"x-user-role\": \"patient\"\n",
    "}\n",
    "\n",
    "r = requests.get(f\"{BASE}/cases/patient/{patient_id}\", headers=headers)\n",
    "print(\"Status:\", r.status_code)\n",
    "print(r.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0293e956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: 200\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "doctor_id = \"358f00bb-d2f3-4352-b9bd-29042d3e9c57\"\n",
    "\n",
    "headers = {\n",
    "    \"x-user-id\": doctor_id,\n",
    "    \"x-user-role\": \"doctor\"\n",
    "}\n",
    "\n",
    "r = requests.get(f\"{BASE}/cases/unassigned\", headers=headers)\n",
    "print(\"Status:\", r.status_code)\n",
    "print(r.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6bef67d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: 200\n",
      "{'status': 'assigned', 'assignment': {'id': 'd6326baf-feca-40f4-891b-81bf7d465d82', 'scan_id': '99d04c3e-c389-4dd3-9939-a78029f69886', 'doctor_id': '358f00bb-d2f3-4352-b9bd-29042d3e9c57', 'accepted_at': '2025-11-30T18:50:47.636607+00:00', 'status': 'assigned'}}\n"
     ]
    }
   ],
   "source": [
    "case_id = \"99d04c3e-c389-4dd3-9939-a78029f69886\"\n",
    "\n",
    "headers = {\n",
    "    \"x-user-id\": doctor_id,\n",
    "    \"x-user-role\": \"doctor\"\n",
    "}\n",
    "\n",
    "r = requests.post(f\"{BASE}/doctor/accept/{case_id}\", headers=headers)\n",
    "print(\"Status:\", r.status_code)\n",
    "print(r.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f54751c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: 400\n",
      "{'detail': 'Case already accepted: {\\'code\\': \\'23505\\', \\'details\\': \\'Key (scan_id)=(99d04c3e-c389-4dd3-9939-a78029f69886) already exists.\\', \\'hint\\': None, \\'message\\': \\'duplicate key value violates unique constraint \"doctor_assignments_scan_id_key\"\\'}'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "BASE = \"http://127.0.0.1:8000\"\n",
    "\n",
    "doctor_id = \"358f00bb-d2f3-4352-b9bd-29042d3e9c57\"\n",
    "case_id = \"99d04c3e-c389-4dd3-9939-a78029f69886\"  # example\n",
    "\n",
    "headers = {\n",
    "    \"x-user-id\": doctor_id,\n",
    "    \"x-user-role\": \"doctor\"\n",
    "}\n",
    "\n",
    "r = requests.post(f\"{BASE}/doctor/accept/{case_id}\", headers=headers)\n",
    "print(\"Status:\", r.status_code)\n",
    "print(r.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a843f335",
   "metadata": {},
   "outputs": [],
   "source": [
    "doctor_id = \"358f00bb-d2f3-4352-b9bd-29042d3e9c57\"\n",
    "case_id    = \"99d04c3e-c389-4dd3-9939-a78029f69886\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f0abc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: 200\n",
      "{'id': 'd6326baf-feca-40f4-891b-81bf7d465d82', 'scan_id': '99d04c3e-c389-4dd3-9939-a78029f69886', 'doctor_id': '358f00bb-d2f3-4352-b9bd-29042d3e9c57', 'accepted_at': '2025-11-30T18:50:47.636607+00:00', 'status': 'assigned'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "BASE = \"http://127.0.0.1:8000\"\n",
    "\n",
    "headers = {\n",
    "    \"x-user-id\": \"358f00bb-d2f3-4352-b9bd-29042d3e9c57\",\n",
    "    \"x-user-role\": \"doctor\"\n",
    "}\n",
    "\n",
    "r = requests.get(f\"{BASE}/doctor/assignment/{case_id}\", headers=headers)\n",
    "print(\"Status:\", r.status_code)\n",
    "print(r.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "02a02d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 {'id': 'c3f9bec2-d0e2-4e52-915b-c971d28e12ff', 'assignment_id': 'd6326baf-feca-40f4-891b-81bf7d465d82', 'sender_id': '358f00bb-d2f3-4352-b9bd-29042d3e9c57', 'message': 'Hello, patient. I have reviewed your CT scan.', 'attachment_url': None, 'sent_at': '2025-11-30T19:20:05.124541+00:00'}\n"
     ]
    }
   ],
   "source": [
    "assignment_id = \"d6326baf-feca-40f4-891b-81bf7d465d82\"\n",
    "\n",
    "headers = {\n",
    "    \"x-user-id\": \"358f00bb-d2f3-4352-b9bd-29042d3e9c57\",\n",
    "    \"x-user-role\": \"doctor\"\n",
    "}\n",
    "\n",
    "payload = {\"message\": \"Hello, patient. I have reviewed your CT scan.\"}\n",
    "\n",
    "r = requests.post(f\"{BASE}/chat/send/{assignment_id}\", headers=headers, json=payload)\n",
    "print(r.status_code, r.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e022398d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: 200\n",
      "[{'id': 'c3f9bec2-d0e2-4e52-915b-c971d28e12ff', 'assignment_id': 'd6326baf-feca-40f4-891b-81bf7d465d82', 'sender_id': '358f00bb-d2f3-4352-b9bd-29042d3e9c57', 'message': 'Hello, patient. I have reviewed your CT scan.', 'attachment_url': None, 'sent_at': '2025-11-30T19:20:05.124541+00:00'}]\n"
     ]
    }
   ],
   "source": [
    "assignment_id = \"d6326baf-feca-40f4-891b-81bf7d465d82\"\n",
    "\n",
    "headers = {\n",
    "    \"x-user-id\": \"5e5202af-44ad-4994-9515-ba2fb3eb4dec\",  # patient\n",
    "    \"x-user-role\": \"patient\"\n",
    "}\n",
    "\n",
    "r = requests.get(f\"{BASE}/chat/history/{assignment_id}\", headers=headers)\n",
    "print(\"Status:\", r.status_code)\n",
    "print(r.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3cc6bf51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "[{'id': 'c3f9bec2-d0e2-4e52-915b-c971d28e12ff', 'assignment_id': 'd6326baf-feca-40f4-891b-81bf7d465d82', 'sender_id': '358f00bb-d2f3-4352-b9bd-29042d3e9c57', 'message': 'Hello, patient. I have reviewed your CT scan.', 'attachment_url': None, 'sent_at': '2025-11-30T19:20:05.124541+00:00'}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "BASE = \"http://127.0.0.1:8000\"\n",
    "\n",
    "patient_id = \"5e5202af-44ad-4994-9515-ba2fb3eb4dec\"\n",
    "assignment_id = \"d6326baf-feca-40f4-891b-81bf7d465d82\"\n",
    "\n",
    "headers = {\n",
    "    \"x-user-id\": patient_id,\n",
    "    \"x-user-role\": \"patient\"\n",
    "}\n",
    "\n",
    "r = requests.get(f\"{BASE}/chat/history/{assignment_id}\", headers=headers)\n",
    "print(r.status_code)\n",
    "print(r.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "68fd3922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 {'id': '9b1d8b29-e794-41e9-9a96-66842e6fa447', 'assignment_id': 'd6326baf-feca-40f4-891b-81bf7d465d82', 'sender_id': '358f00bb-d2f3-4352-b9bd-29042d3e9c57', 'message': 'Please drink more water.', 'attachment_url': None, 'sent_at': '2025-11-30T19:32:34.666245+00:00'}\n"
     ]
    }
   ],
   "source": [
    "doctor_id = \"358f00bb-d2f3-4352-b9bd-29042d3e9c57\"\n",
    "\n",
    "headers = {\n",
    "    \"x-user-id\": doctor_id,\n",
    "    \"x-user-role\": \"doctor\"\n",
    "}\n",
    "\n",
    "payload = {\"message\": \"Please drink more water.\"}\n",
    "\n",
    "r = requests.post(\n",
    "    f\"{BASE}/chat/send/{assignment_id}\",\n",
    "    headers=headers,\n",
    "    json=payload\n",
    ")\n",
    "\n",
    "print(r.status_code, r.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1b497626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 {'id': '0ca99eb0-ac59-4541-8bfc-991082fea2dc', 'assignment_id': 'd6326baf-feca-40f4-891b-81bf7d465d82', 'sender_id': '5e5202af-44ad-4994-9515-ba2fb3eb4dec', 'message': 'Thank you doctor!', 'attachment_url': None, 'sent_at': '2025-11-30T19:32:42.075115+00:00'}\n"
     ]
    }
   ],
   "source": [
    "headers = {\n",
    "    \"x-user-id\": patient_id,\n",
    "    \"x-user-role\": \"patient\"\n",
    "}\n",
    "\n",
    "payload = {\"message\": \"Thank you doctor!\"}\n",
    "\n",
    "r = requests.post(\n",
    "    f\"{BASE}/chat/send/{assignment_id}\",\n",
    "    headers=headers,\n",
    "    json=payload\n",
    ")\n",
    "\n",
    "print(r.status_code, r.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "75225e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "[{'id': 'c3f9bec2-d0e2-4e52-915b-c971d28e12ff', 'assignment_id': 'd6326baf-feca-40f4-891b-81bf7d465d82', 'sender_id': '358f00bb-d2f3-4352-b9bd-29042d3e9c57', 'message': 'Hello, patient. I have reviewed your CT scan.', 'attachment_url': None, 'sent_at': '2025-11-30T19:20:05.124541+00:00'}, {'id': '9b1d8b29-e794-41e9-9a96-66842e6fa447', 'assignment_id': 'd6326baf-feca-40f4-891b-81bf7d465d82', 'sender_id': '358f00bb-d2f3-4352-b9bd-29042d3e9c57', 'message': 'Please drink more water.', 'attachment_url': None, 'sent_at': '2025-11-30T19:32:34.666245+00:00'}, {'id': '0ca99eb0-ac59-4541-8bfc-991082fea2dc', 'assignment_id': 'd6326baf-feca-40f4-891b-81bf7d465d82', 'sender_id': '5e5202af-44ad-4994-9515-ba2fb3eb4dec', 'message': 'Thank you doctor!', 'attachment_url': None, 'sent_at': '2025-11-30T19:32:42.075115+00:00'}]\n"
     ]
    }
   ],
   "source": [
    "headers = {\n",
    "    \"x-user-id\": patient_id,\n",
    "    \"x-user-role\": \"patient\"\n",
    "}\n",
    "\n",
    "r = requests.get(f\"{BASE}/chat/history/{assignment_id}\", headers=headers)\n",
    "print(r.status_code)\n",
    "print(r.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14706b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPLOAD: 200 {'case_id': '9bd6694d-03ba-46ed-9637-18d7b0a2be53', 'storage_path': '9bd6694d-03ba-46ed-9637-18d7b0a2be53/test_ct_extracted.zip'}\n",
      "PROCESS: 500 {\"detail\":\"Pipeline error: Pipeline did not produce findings.json\"}\n",
      "CASE: 200 {'id': '9bd6694d-03ba-46ed-9637-18d7b0a2be53', 'patient_id': '5e5202af-44ad-4994-9515-ba2fb3eb4dec', 'uploaded_at': '2025-11-30T21:53:29.759283+00:00', 'storage_path': '9bd6694d-03ba-46ed-9637-18d7b0a2be53/test_ct_extracted.zip', 'status': 'failed', 'updated_at': '2025-11-30T21:53:49.711539+00:00'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "BASE = \"http://127.0.0.1:8000\"\n",
    "\n",
    "patient_id = \"5e5202af-44ad-4994-9515-ba2fb3eb4dec\"\n",
    "operator_id = \"6255e461-2a14-4b3c-a635-de4428668fc0\"\n",
    "doctor_id = \"358f00bb-d2f3-4352-b9bd-29042d3e9c57\"\n",
    "\n",
    "# ---------------------------------------\n",
    "# STEP 1 — UPLOAD ZIP AS PATIENT\n",
    "# ---------------------------------------\n",
    "files = {\"file\": open(\"test_ct_extracted.zip\", \"rb\")}\n",
    "headers_patient = {\"x-user-id\": patient_id, \"x-user-role\": \"patient\"}\n",
    "\n",
    "r = requests.post(f\"{BASE}/upload/scan\", headers=headers_patient, files=files)\n",
    "print(\"UPLOAD:\", r.status_code, r.json())\n",
    "\n",
    "case_id = r.json()[\"case_id\"]\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# STEP 2 — OPERATOR PROCESS CASE\n",
    "# ---------------------------------------\n",
    "headers_operator = {\"x-user-id\": operator_id, \"x-user-role\": \"operator\"}\n",
    "\n",
    "r = requests.post(f\"{BASE}/process/case/{case_id}\", headers=headers_operator)\n",
    "print(\"PROCESS:\", r.status_code, r.text)\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# STEP 3 — CHECK ml_json BUCKET VIA DB\n",
    "# ---------------------------------------\n",
    "headers_patient = {\"x-user-id\": patient_id, \"x-user-role\": \"patient\"}\n",
    "\n",
    "r = requests.get(f\"{BASE}/cases/{case_id}\", headers=headers_patient)\n",
    "print(\"CASE:\", r.status_code, r.json())\n",
    "\n",
    "# CASE OUTPUT SHOULD SHOW:\n",
    "# \"json_path\": \"case_id/findings.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca7f4fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== UPLOAD ===\n",
      "Status: 200\n",
      "{\n",
      "  \"case_id\": \"5a4f4c70-db98-4ea5-ba84-e6d62776d285\",\n",
      "  \"storage_path\": \"5a4f4c70-db98-4ea5-ba84-e6d62776d285/test_ct_extracted.zip\"\n",
      "}\n",
      "\n",
      "Case ID: 5a4f4c70-db98-4ea5-ba84-e6d62776d285\n",
      "\n",
      "Running pipeline... (This may take 30–120 seconds)\n",
      "\n",
      "=== PROCESS ===\n",
      "Status: 200\n",
      "{\n",
      "  \"status\": \"completed\"\n",
      "}\n",
      "\n",
      "=== CASE DETAILS ===\n",
      "Status: 200\n",
      "{\n",
      "  \"id\": \"5a4f4c70-db98-4ea5-ba84-e6d62776d285\",\n",
      "  \"patient_id\": \"5e5202af-44ad-4994-9515-ba2fb3eb4dec\",\n",
      "  \"uploaded_at\": \"2025-11-30T22:37:17.973716+00:00\",\n",
      "  \"storage_path\": \"5a4f4c70-db98-4ea5-ba84-e6d62776d285/test_ct_extracted.zip\",\n",
      "  \"status\": \"processing\",\n",
      "  \"updated_at\": \"2025-11-30T22:37:26.729208+00:00\"\n",
      "}\n",
      "\n",
      "=== SCAN RESULTS ===\n",
      "Status: 404\n",
      "{\n",
      "  \"detail\": \"Not Found\"\n",
      "}\n",
      "\n",
      "=== UNASSIGNED CASES ===\n",
      "Status: 200\n",
      "[]\n",
      "\n",
      "=== DOCTOR ACCEPTS CASE ===\n",
      "Status: 200\n",
      "{\n",
      "  \"id\": \"cdbdc14b-3131-44a9-9c00-16455aa38dc0\",\n",
      "  \"scan_id\": \"5a4f4c70-db98-4ea5-ba84-e6d62776d285\",\n",
      "  \"doctor_id\": \"358f00bb-d2f3-4352-b9bd-29042d3e9c57\",\n",
      "  \"status\": \"assigned\",\n",
      "  \"accepted_at\": \"2025-11-30T22:37:46.163603+00:00\"\n",
      "}\n",
      "\n",
      "Assignment ID: cdbdc14b-3131-44a9-9c00-16455aa38dc0\n",
      "\n",
      "=== DOCTOR SENDS MESSAGE ===\n",
      "Status: 200\n",
      "{\n",
      "  \"id\": \"1e633dee-1774-4c5a-bf13-f7094e7db9ec\",\n",
      "  \"assignment_id\": \"cdbdc14b-3131-44a9-9c00-16455aa38dc0\",\n",
      "  \"sender_id\": \"358f00bb-d2f3-4352-b9bd-29042d3e9c57\",\n",
      "  \"message\": \"Your CT scan report is ready. Please check your results.\",\n",
      "  \"attachment_url\": null,\n",
      "  \"sent_at\": \"2025-11-30T22:37:46.662459+00:00\"\n",
      "}\n",
      "\n",
      "=== CHAT HISTORY ===\n",
      "Status: 200\n",
      "[\n",
      "  {\n",
      "    \"id\": \"1e633dee-1774-4c5a-bf13-f7094e7db9ec\",\n",
      "    \"assignment_id\": \"cdbdc14b-3131-44a9-9c00-16455aa38dc0\",\n",
      "    \"sender_id\": \"358f00bb-d2f3-4352-b9bd-29042d3e9c57\",\n",
      "    \"message\": \"Your CT scan report is ready. Please check your results.\",\n",
      "    \"attachment_url\": null,\n",
      "    \"sent_at\": \"2025-11-30T22:37:46.662459+00:00\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "BASE = \"http://127.0.0.1:8000\"\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# CONFIGURE USER IDS\n",
    "# ---------------------------------------------------------\n",
    "PATIENT_ID = \"5e5202af-44ad-4994-9515-ba2fb3eb4dec\"\n",
    "OPERATOR_ID = \"6255e461-2a14-4b3c-a635-de4428668fc0\"\n",
    "DOCTOR_ID = \"358f00bb-d2f3-4352-b9bd-29042d3e9c57\"\n",
    "\n",
    "ZIP_FILE = \"test_ct_extracted.zip\"   # <== set your test ZIP file here\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "def pp(title, res):\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    print(\"Status:\", res.status_code)\n",
    "    try:\n",
    "        print(json.dumps(res.json(), indent=2))\n",
    "    except:\n",
    "        print(res.text)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1) PATIENT UPLOADS CT ZIP\n",
    "# ---------------------------------------------------------\n",
    "files = {\"file\": open(ZIP_FILE, \"rb\")}\n",
    "headers = {\"x-user-id\": PATIENT_ID, \"x-user-role\": \"patient\"}\n",
    "\n",
    "res = requests.post(f\"{BASE}/upload/scan\", headers=headers, files=files)\n",
    "pp(\"UPLOAD\", res)\n",
    "\n",
    "case_id = res.json()[\"case_id\"]\n",
    "print(\"\\nCase ID:\", case_id)\n",
    "\n",
    "print(\"\\nRunning pipeline... (This may take 30–120 seconds)\")\n",
    "time.sleep(2)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2) OPERATOR PROCESSES SCAN\n",
    "# ---------------------------------------------------------\n",
    "headers = {\"x-user-id\": OPERATOR_ID, \"x-user-role\": \"operator\"}\n",
    "\n",
    "res = requests.post(f\"{BASE}/process/case/{case_id}\", headers=headers)\n",
    "pp(\"PROCESS\", res)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3) CHECK CASE DETAILS\n",
    "# ---------------------------------------------------------\n",
    "headers = {\"x-user-id\": PATIENT_ID, \"x-user-role\": \"patient\"}\n",
    "\n",
    "res = requests.get(f\"{BASE}/cases/{case_id}\", headers=headers)\n",
    "pp(\"CASE DETAILS\", res)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4) CHECK FINDINGS JSON (scan_results table)\n",
    "# ---------------------------------------------------------\n",
    "res = requests.get(f\"{BASE}/scan_results/{case_id}\", headers=headers)\n",
    "pp(\"SCAN RESULTS\", res)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5) DOCTOR FETCHES UNASSIGNED CASES\n",
    "# ---------------------------------------------------------\n",
    "headers = {\"x-user-id\": DOCTOR_ID, \"x-user-role\": \"doctor\"}\n",
    "\n",
    "res = requests.get(f\"{BASE}/cases/unassigned\", headers=headers)\n",
    "pp(\"UNASSIGNED CASES\", res)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6) DOCTOR ACCEPTS CASE\n",
    "# ---------------------------------------------------------\n",
    "res = requests.post(f\"{BASE}/doctor/accept/{case_id}\", headers=headers)\n",
    "pp(\"DOCTOR ACCEPTS CASE\", res)\n",
    "\n",
    "assignment_id = res.json().get(\"id\")   # depends on API return\n",
    "print(\"\\nAssignment ID:\", assignment_id)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 7) DOCTOR SENDS CHAT MESSAGE\n",
    "# ---------------------------------------------------------\n",
    "payload = {\"message\": \"Your CT scan report is ready. Please check your results.\"}\n",
    "\n",
    "res = requests.post(\n",
    "    f\"{BASE}/chat/send/{assignment_id}\",\n",
    "    headers=headers,\n",
    "    json=payload\n",
    ")\n",
    "pp(\"DOCTOR SENDS MESSAGE\", res)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 8) PATIENT FETCHES CHAT HISTORY\n",
    "# ---------------------------------------------------------\n",
    "headers = {\"x-user-id\": PATIENT_ID, \"x-user-role\": \"patient\"}\n",
    "\n",
    "res = requests.get(f\"{BASE}/chat/history/{assignment_id}\", headers=headers)\n",
    "pp(\"CHAT HISTORY\", res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20e7b76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fypenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
