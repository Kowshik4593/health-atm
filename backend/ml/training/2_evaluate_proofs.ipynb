{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 2. Evaluation & Proofs\n",
                "\n",
                "This notebook generates the final proofs for the dissertation/report.\n",
                "\n",
                "**Objectives:**\n",
                "- Calculate **Dice Score** (Segmentation Accuracy)\n",
                "- Calculate **AUC-ROC** (Malignancy Classification Performance)\n",
                "- Visualization: **Input vs. Ground Truth vs. Prediction**\n",
                "- **GradCAM++** Heatmaps for XAI"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import numpy as np\n",
                "from pathlib import Path\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.metrics import roc_curve, auc, confusion_matrix, accuracy_score\n",
                "import seaborn as sns\n",
                "import glob\n",
                "import cv2\n",
                "from tqdm.notebook import tqdm\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Model & Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Re-define Model (Must match training definition)\n",
                "class DoubleConv(nn.Module):\n",
                "    def __init__(self, in_channels, out_channels):\n",
                "        super().__init__()\n",
                "        self.conv = nn.Sequential(\n",
                "            nn.Conv3d(in_channels, out_channels, 3, padding=1),\n",
                "            nn.BatchNorm3d(out_channels),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Conv3d(out_channels, out_channels, 3, padding=1),\n",
                "            nn.BatchNorm3d(out_channels),\n",
                "            nn.ReLU(inplace=True)\n",
                "        )\n",
                "\n",
                "    def forward(self, x):\n",
                "        return self.conv(x)\n",
                "\n",
                "class LightweightUNet3D(nn.Module):\n",
                "    def __init__(self, in_channels=1, out_channels=1):\n",
                "        super().__init__()\n",
                "        self.inc = DoubleConv(in_channels, 16)\n",
                "        self.down1 = nn.Sequential(nn.MaxPool3d(2), DoubleConv(16, 32))\n",
                "        self.down2 = nn.Sequential(nn.MaxPool3d(2), DoubleConv(32, 64))\n",
                "        self.down3 = nn.Sequential(nn.MaxPool3d(2), DoubleConv(64, 128))\n",
                "        self.up1 = nn.ConvTranspose3d(128, 64, 2, stride=2)\n",
                "        self.conv1 = DoubleConv(128, 64)\n",
                "        self.up2 = nn.ConvTranspose3d(64, 32, 2, stride=2)\n",
                "        self.conv2 = DoubleConv(64, 32)\n",
                "        self.up3 = nn.ConvTranspose3d(32, 16, 2, stride=2)\n",
                "        self.conv3 = DoubleConv(32, 16)\n",
                "        self.outc = nn.Conv3d(16, out_channels, 1)\n",
                "        self.classifier = nn.Sequential(\n",
                "            nn.AdaptiveAvgPool3d(1),\n",
                "            nn.Flatten(),\n",
                "            nn.Linear(128, 32),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(32, 1)\n",
                "        )\n",
                "\n",
                "    def forward(self, x):\n",
                "        x1 = self.inc(x)\n",
                "        x2 = self.down1(x1)\n",
                "        x3 = self.down2(x2)\n",
                "        x4 = self.down3(x3)\n",
                "        u1 = self.up1(x4)\n",
                "        u1 = torch.cat([u1, x3], dim=1)\n",
                "        u1 = self.conv1(u1)\n",
                "        u2 = self.up2(u1)\n",
                "        u2 = torch.cat([u2, x2], dim=1)\n",
                "        u2 = self.conv2(u2)\n",
                "        u3 = self.up3(u2)\n",
                "        u3 = torch.cat([u3, x1], dim=1)\n",
                "        u3 = self.conv3(u3)\n",
                "        mask = torch.sigmoid(self.outc(u3))\n",
                "        risk = torch.sigmoid(self.classifier(x4))\n",
                "        return mask, risk\n",
                "\n",
                "model = LightweightUNet3D().to(device)\n",
                "try:\n",
                "    model.load_state_dict(torch.load(\"../models/unet3d_lightweight.pth\", map_location=device))\n",
                "    print(\"Model loaded successfully!\")\n",
                "except FileNotFoundError:\n",
                "    print(\"Model file not found. Please train first.\")\n",
                "model.eval()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def dice_score(pred, target, smooth=1e-5):\n",
                "    pred = pred.reshape(-1)\n",
                "    target = target.reshape(-1)\n",
                "    intersection = (pred * target).sum()\n",
                "    return (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n",
                "\n",
                "DATA_DIR = Path(\"../data/lidc_patches\")\n",
                "test_files = glob.glob(str(DATA_DIR / \"*.npz\")) # In reality, split into train/test\n",
                "print(f\"Evaluating on {len(test_files)} samples.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Evaluation Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_true_cls = []\n",
                "y_pred_cls = []\n",
                "dice_scores = []\n",
                "\n",
                "samples_for_viz = []\n",
                "\n",
                "with torch.no_grad():\n",
                "    for f in tqdm(test_files):\n",
                "        data = np.load(f)\n",
                "        \n",
                "        # Prepare Input\n",
                "        image = data['image'].astype(np.float32)\n",
                "        image = (image - (-1000)) / 1400.0\n",
                "        image = np.clip(image, 0, 1)\n",
                "        image_tensor = torch.tensor(image).unsqueeze(0).unsqueeze(0).to(device)\n",
                "        \n",
                "        # Prepare label\n",
                "        label = data['label']\n",
                "        \n",
                "         # Prepare mask\n",
                "        if 'mask' in data:\n",
                "            mask = data['mask'].astype(np.float32)\n",
                "        else:\n",
                "             # Synthetic for demo\n",
                "            mask = np.zeros((64, 64, 64), dtype=np.float32)\n",
                "            z, y, x = np.ogrid[:64, :64, :64]\n",
                "            mask[((z-32)**2 + (y-32)**2 + (x-32)**2 <= 100)] = 1.0\n",
                "            \n",
                "        # Inference\n",
                "        pred_mask, pred_risk = model(image_tensor)\n",
                "        \n",
                "        # Classification Metrics\n",
                "        y_true_cls.append(label)\n",
                "        y_pred_cls.append(pred_risk.item())\n",
                "        \n",
                "        # Segmentation Metrics\n",
                "        pred_mask_np = pred_mask.cpu().numpy().squeeze()\n",
                "        binary_mask = (pred_mask_np > 0.5).astype(np.float32)\n",
                "        dice = dice_score(binary_mask, mask)\n",
                "        dice_scores.append(dice)\n",
                "        \n",
                "        if len(samples_for_viz) < 5:\n",
                "            samples_for_viz.append((image, mask, pred_mask_np))\n",
                "\n",
                "print(f\"Average Dice Score: {np.mean(dice_scores):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Visualization: ROC Curve & Confusion Matrix"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ROC Curve\n",
                "if len(np.unique(y_true_cls)) > 1: # Only plot if we have both classes\n",
                "    fpr, tpr, _ = roc_curve(y_true_cls, y_pred_cls)\n",
                "    roc_auc = auc(fpr, tpr)\n",
                "\n",
                "    plt.figure()\n",
                "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
                "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
                "    plt.xlim([0.0, 1.0])\n",
                "    plt.ylim([0.0, 1.05])\n",
                "    plt.xlabel('False Positive Rate')\n",
                "    plt.ylabel('True Positive Rate')\n",
                "    plt.title('Receiver Operating Characteristic')\n",
                "    plt.legend(loc=\"lower right\")\n",
                "    plt.show()\n",
                "else:\n",
                "    print(\"Not enough classes to plot ROC (Need both 0 and 1).\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Visual Proofs: Segmentation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for i, (img, gt, pred) in enumerate(samples_for_viz):\n",
                "    # Take center slice\n",
                "    mid = 32\n",
                "    \n",
                "    plt.figure(figsize=(15, 5))\n",
                "    \n",
                "    plt.subplot(1, 3, 1)\n",
                "    plt.title(\"Input CT Slice\")\n",
                "    plt.imshow(img[mid], cmap='gray')\n",
                "    plt.axis('off')\n",
                "    \n",
                "    plt.subplot(1, 3, 2)\n",
                "    plt.title(\"Ground Truth Mask\")\n",
                "    plt.imshow(gt[mid], cmap='gray')\n",
                "    plt.axis('off')\n",
                "    \n",
                "    plt.subplot(1, 3, 3)\n",
                "    plt.title(\"Predicted Segmentation\")\n",
                "    plt.imshow(pred[mid], cmap='jet', alpha=0.9)\n",
                "    plt.axis('off')\n",
                "    \n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. GradCAM++ Visualization\n",
                "\n",
                "We verify that the network is looking at the nodule."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simple GradCAM Hook\n",
                "gradients = None\n",
                "activations = None\n",
                "\n",
                "def backward_hook(module, grad_input, grad_output):\n",
                "    global gradients\n",
                "    gradients = grad_output[0]\n",
                "\n",
                "def forward_hook(module, input, output):\n",
                "    global activations\n",
                "    activations = output\n",
                "\n",
                "# Hook onto the last encoder layer (bottleneck)\n",
                "target_layer = model.down3[1].conv[5] # Last conv in down3\n",
                "target_layer.register_forward_hook(forward_hook)\n",
                "target_layer.register_backward_hook(backward_hook)\n",
                "\n",
                "# Run one inference\n",
                "img, _, _ = samples_for_viz[0]\n",
                "inputs = torch.tensor(img).unsqueeze(0).unsqueeze(0).to(device, requires_grad=True)\n",
                "_, risk = model(inputs)\n",
                "\n",
                "# Backward\n",
                "model.zero_grad()\n",
                "risk.backward()\n",
                "\n",
                "# Generate CAM\n",
                "weights = torch.mean(gradients, dim=(2, 3, 4), keepdim=True)\n",
                "cam = torch.sum(weights * activations, dim=1, keepdim=True)\n",
                "cam = torch.relu(cam)\n",
                "cam = cam.squeeze().cpu().detach().numpy()\n",
                "\n",
                "# Resize CAM to input size\n",
                "cam = cv2.resize(cam[4], (64, 64)) # Take middle feature map slice and resize\n",
                "plt.figure()\n",
                "plt.title(\"GradCAM Activation (Center Slice)\")\n",
                "plt.imshow(cam, cmap='jet')\n",
                "plt.colorbar()\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}